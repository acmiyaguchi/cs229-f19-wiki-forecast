{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:///Users/acmiy/OneDrive/Documents/cs229/cs229-f19-wiki-forecast/data/tmp\n"
     ]
    }
   ],
   "source": [
    "# optimization to save on shuffle space\n",
    "spark.conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "\n",
    "# this needs to be set to a reasonable number on a local machine\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"16\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "# hack to provide a posix like path in Windows\n",
    "from pathlib import Path\n",
    "checkpoint_dir = \"file:///\" + \"/\".join(\n",
    "    (Path(\"..\").resolve() / \"data/tmp\").parts[1:]\n",
    ")\n",
    "print(checkpoint_dir)\n",
    "\n",
    "# needed by GraphFrames to persist intermediate results to local disk\n",
    "sc.setCheckpointDir(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from graphframes import GraphFrame\n",
    "\n",
    "pages_path = \"../data/enwiki/pages\"\n",
    "pagelinks_path = \"../data/enwiki/pagelinks\"\n",
    "sampling_ratio = 0.02\n",
    "\n",
    "pages = spark.read.parquet(pages_path)\n",
    "pagelinks = spark.read.parquet(pagelinks_path)\n",
    "\n",
    "def sample_graph(pages, pagelinks, sampling_ratio, relabel=True):\n",
    "    vertices = (\n",
    "        pages\n",
    "        .sample(sampling_ratio)\n",
    "        .withColumn(\"rank\", F.expr(\"row_number() over (order by id)\"))\n",
    "        # ensure 0 index for mapping into a scipy.sparse matrix\n",
    "        .withColumn(\"rank\", F.expr(\"rank - 1\"))\n",
    "    )\n",
    "    edges = (\n",
    "        pagelinks.join(\n",
    "            vertices.selectExpr(\"id as from\", \"rank as relabeled_src\"),\n",
    "            on=\"from\", \n",
    "            how=\"inner\"\n",
    "        ).join(\n",
    "            vertices.selectExpr(\"id as dest\", \"rank as relabeled_dst\"), \n",
    "            on=\"dest\", \n",
    "            how=\"inner\"\n",
    "        )\n",
    "    ).selectExpr(\"from as src\", \"dest as dst\", \"relabeled_src\", \"relabeled_dst\")\n",
    "    \n",
    "    if relabel:\n",
    "        vertices = vertices.selectExpr(\"rank as id\", \"id as article_id\")\n",
    "        edges = edges.selectExpr(\"relabeled_src as src\", \"relabeled_dst as dst\")\n",
    "    return GraphFrame(vertices, edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the largest connected component\n",
    "\n",
    "The graph must be fully connected to apply spectral methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def connected_components(pages, pagelinks, sample_ratio):\n",
    "    graph = sample_graph(pages, pagelinks, sample_ratio)\n",
    "    total = graph.vertices.count()\n",
    "    results = (\n",
    "        graph.connectedComponents()\n",
    "        .groupBy(\"component\")\n",
    "        .count()\n",
    "        .orderBy(F.desc(\"count\"))\n",
    "        .withColumn(\"total\", F.lit(total))\n",
    "        .withColumn(\"frac\", F.expr(\"round(count/total*100, 3)\"))\n",
    "    )\n",
    "    results.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------+------+\n",
      "|component|count| total|  frac|\n",
      "+---------+-----+------+------+\n",
      "|        1|59249|118017|50.204|\n",
      "|     8811|   18|118017| 0.015|\n",
      "|    55824|   17|118017| 0.014|\n",
      "|    79400|   16|118017| 0.014|\n",
      "|    41245|   16|118017| 0.014|\n",
      "+---------+-----+------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Wall time: 26.3 s\n",
      "+---------+------+------+------+\n",
      "|component| count| total|  frac|\n",
      "+---------+------+------+------+\n",
      "|        1|212828|296197|71.854|\n",
      "|   109084|    22|296197| 0.007|\n",
      "|   144422|    21|296197| 0.007|\n",
      "|   107609|    19|296197| 0.006|\n",
      "|    98211|    18|296197| 0.006|\n",
      "+---------+------+------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Wall time: 26.9 s\n",
      "+---------+------+------+------+\n",
      "|component| count| total|  frac|\n",
      "+---------+------+------+------+\n",
      "|        1|522102|591992|88.194|\n",
      "|   231613|    15|591992| 0.003|\n",
      "|   188285|    14|591992| 0.002|\n",
      "|    82808|    12|591992| 0.002|\n",
      "|   170378|    11|591992| 0.002|\n",
      "+---------+------+------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Wall time: 33.7 s\n"
     ]
    }
   ],
   "source": [
    "%time connected_components(pages, pagelinks, 0.02)\n",
    "%time connected_components(pages, pagelinks, 0.05)\n",
    "%time connected_components(pages, pagelinks, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this takes 12 minutes to complete, with 99.996% of nodes in the largest component\n",
    "# %time connected_components(pages, pagelinks, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive bi-partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = sample_graph(pages, pagelinks, 0.02)\n",
    "g.cache()\n",
    "edges = g.edges.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|article_id|\n",
      "+---+----------+\n",
      "|  0|       621|\n",
      "|  1|       802|\n",
      "|  2|       888|\n",
      "+---+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------+----------+\n",
      "|    id|article_id|\n",
      "+------+----------+\n",
      "|118605|  61562569|\n",
      "|118604|  61562533|\n",
      "|118603|  61562477|\n",
      "+------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.vertices.orderBy(F.asc(\"id\")).show(n=3)\n",
    "g.vertices.orderBy(F.desc(\"id\")).show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy.sparse.csgraph import laplacian\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sparse_matrix_from_edgelist(edges):\n",
    "    n = edges[\"src\"].max()\n",
    "    ones = np.ones(len(edges[\"dst\"]))\n",
    "    return sp.coo_matrix(\n",
    "        (ones, (edges[\"src\"], edges[\"dst\"])), \n",
    "        shape=(n+1, n+1)\n",
    "    ).tocsr()\n",
    "    \n",
    "def fiedler_vector(g):\n",
    "    L = laplacian(g)\n",
    "    # ordered by smallest eigenvalue\n",
    "    _, v = eigsh(L, k=2)\n",
    "    # fiedler vector is the second smallest eigenvalue\n",
    "    return v[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 270 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(118606,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lap = sparse_matrix_from_edgelist(edges)\n",
    "%time vec = fiedler_vector(lap)\n",
    "vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "@pandas_udf(\"id long, value double\", PandasUDFType.GROUPED_MAP)\n",
    "# Input/output are both a pandas.DataFrame\n",
    "def compute_fiedler(pdf):\n",
    "    g = sparse_matrix_from_edgelist(pdf)\n",
    "    vec = fiedler_vector(g)\n",
    "    return pd.DataFrame([{\"id\": i, \"value\": v} for i, v in enumerate(vec)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|               value|\n",
      "+---+--------------------+\n",
      "|  0|-4.74153474596838...|\n",
      "|  1|1.816576854708336E-7|\n",
      "|  2|2.212099119240315E-7|\n",
      "|  3|-2.27131145561087...|\n",
      "|  4|-5.38874987399268...|\n",
      "+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "results = g.edges.withColumn(\"part_id\", F.lit(0)).groupBy(\"part_id\").apply(compute_fiedler)\n",
    "%time results.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| part|count|\n",
      "+-----+-----+\n",
      "| true|58882|\n",
      "|false|59724|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.selectExpr(\"value >= 0 as part\").groupBy(\"part\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def induce_graph(graph, relabel=True):\n",
    "    \"\"\"Remove extra edges that do not belong to the graph\"\"\"\n",
    "    # small dataframe for reindexing/relabeling\n",
    "    rank = (\n",
    "        graph.vertices\n",
    "        .select(\"id\", F.expr(\"row_number() over (order by id)\").alias(\"rank\"))\n",
    "        # ensure 0 index for mapping into a scipy.sparse matrix\n",
    "        .withColumn(\"rank\", F.expr(\"rank - 1\"))\n",
    "    )\n",
    "    vertices = graph.vertices.join(rank, on=\"id\", how=\"inner\")\n",
    "    edges = (\n",
    "        graph.edges.join(\n",
    "            vertices.selectExpr(\"id as src\", \"rank as relabeled_src\"),\n",
    "            on=\"src\", \n",
    "            how=\"inner\"\n",
    "        ).join(\n",
    "            vertices.selectExpr(\"id as dst\", \"rank as relabeled_dst\"), \n",
    "            on=\"dst\", \n",
    "            how=\"inner\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if relabel:\n",
    "        vertices = (\n",
    "            vertices\n",
    "            .withColumn(\"original_id\", F.col(\"id\"))\n",
    "            .withColumn(\"id\", F.col(\"rank\"))\n",
    "        )\n",
    "        edges = edges.selectExpr(\"relabeled_src as src\", \"relabeled_dst as dst\")\n",
    "    vertices = vertices.drop(\"rank\")\n",
    "    edges = edges.drop(\"relabeled_src\").drop(\"relabeled_dst\")\n",
    "\n",
    "    return GraphFrame(vertices, edges)\n",
    "\n",
    "\n",
    "def recursive_bipartition(graph: GraphFrame, max_iter: int = 2) -> GraphFrame:\n",
    "    def undo_relabel(vertices):\n",
    "        return vertices.withColumn(\"id\", F.col(\"original_id\")).drop(\"original_id\")\n",
    "\n",
    "    \"\"\"\n",
    "    Assumes the input graph has been relabeled, which is required for performance of\n",
    "    scipy.linalg.eigsh.\n",
    "    \"\"\"\n",
    "    def bipartition(graph: GraphFrame, partitions: List[str] = [], iteration: int = 0):\n",
    "        \n",
    "        partition = f\"sign_{iteration}\"\n",
    "        fiedler_value = f\"fiedler_{iteration}\"\n",
    "        fiedler = (\n",
    "            graph\n",
    "            .edges\n",
    "            # necessary for collecting all edges to a single worker\n",
    "            .withColumn(\"part\", F.lit(True))\n",
    "            .groupBy(\"part\")\n",
    "            .apply(compute_fiedler)\n",
    "            .withColumn(partition, F.expr(\"value >= 0\").astype(\"boolean\"))\n",
    "            .selectExpr(\"id\", f\"value as {fiedler_value}\", partition)\n",
    "        )\n",
    "        \n",
    "        # NOTE: assumes relabeling, reverse the relabeling process\n",
    "        vertices = graph.vertices.join(fiedler, on=\"id\", how=\"left\")\n",
    "        parted_graph = GraphFrame(vertices, graph.edges)\n",
    "        \n",
    "        if iteration == max_iter:\n",
    "            return undo_relabel(parted_graph.vertices)\n",
    "        else:\n",
    "            positive_vertices = bipartition(\n",
    "                induce_graph(\n",
    "                    GraphFrame(\n",
    "                        parted_graph.vertices.where(f\"{partition}\"),\n",
    "                        parted_graph.edges,\n",
    "                    ),\n",
    "                    relabel=True,\n",
    "                ),\n",
    "                partitions + [partition],\n",
    "                iteration + 1,\n",
    "            )\n",
    "            negative_vertices = bipartition(\n",
    "                induce_graph(\n",
    "                    GraphFrame(\n",
    "                        parted_graph.vertices.where(f\"NOT {partition}\"),\n",
    "                        parted_graph.edges,\n",
    "                    ),\n",
    "                    relabel=True,\n",
    "                ),\n",
    "                partitions + [partition],\n",
    "                iteration + 1,\n",
    "            )\n",
    "            # reusing the index, should this be saved?\n",
    "            return undo_relabel(\n",
    "                positive_vertices\n",
    "                .union(negative_vertices)\n",
    "                .join(vertices.select(\"id\", \"original_id\"), on=\"id\", how=\"inner\")\n",
    "            )\n",
    "\n",
    "    return bipartition(induce_graph(graph, relabel=True))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+------+--------------------+------+--------------------+------+\n",
      "| id|article_id|           fiedler_0|sign_0|           fiedler_1|sign_1|           fiedler_2|sign_2|\n",
      "+---+----------+--------------------+------+--------------------+------+--------------------+------+\n",
      "| 12|      1915|-3.77200658910386...| false|-3.84364987273148...| false|-1.62689614330089...| false|\n",
      "| 18|      2439|-2.51943472764536...| false|-8.78679236681460...| false|-8.18238274138275...| false|\n",
      "| 38|      4667|-4.11143290197177E-6| false|-1.46728074632263...| false|-2.02239415597500...| false|\n",
      "| 67|      7774|-8.78638934628648...| false|-3.93770260955373...| false|-1.65094696383655...| false|\n",
      "| 70|      8103|8.751971287319299E-4|  true|-3.61108512861437...| false|-1.25315827966449...| false|\n",
      "+---+----------+--------------------+------+--------------------+------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Wall time: 3min 31s\n"
     ]
    }
   ],
   "source": [
    "result = recursive_bipartition(g)\n",
    "%time result.show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+-----+\n",
      "|sign_0|sign_1|sign_2|count|\n",
      "+------+------+------+-----+\n",
      "| false| false| false|15279|\n",
      "|  true|  true| false|14578|\n",
      "| false|  true| false|15149|\n",
      "|  true| false|  true|14595|\n",
      "|  true|  true|  true|14584|\n",
      "|  true|  true|  null|   15|\n",
      "|  true| false| false|14486|\n",
      "| false| false|  true|15246|\n",
      "| false|  true|  true|14983|\n",
      "|  true| false|  null|    4|\n",
      "| false| false|  null|    2|\n",
      "+------+------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "signs = [c for c in result.columns if c.startswith(\"sign_\")]\n",
    "result.groupBy(*signs).count().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
