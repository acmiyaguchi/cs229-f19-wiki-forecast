{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/amiyaguchi/Research/wiki-forecast\n",
      "Requirement already satisfied: click in /Users/amiyaguchi/.local/share/virtualenvs/wiki-forecast-HU65IRGn/lib/python3.6/site-packages (from wiki-forecast==0.3.3) (7.0)\n",
      "Requirement already satisfied: pyspark>=2.4.0 in /Users/amiyaguchi/.local/share/virtualenvs/wiki-forecast-HU65IRGn/lib/python3.6/site-packages (from wiki-forecast==0.3.3) (2.4.4)\n",
      "Requirement already satisfied: networkx in /Users/amiyaguchi/.local/share/virtualenvs/wiki-forecast-HU65IRGn/lib/python3.6/site-packages (from wiki-forecast==0.3.3) (2.3)\n",
      "Requirement already satisfied: matplotlib in /Users/amiyaguchi/.local/share/virtualenvs/wiki-forecast-HU65IRGn/lib/python3.6/site-packages (from wiki-forecast==0.3.3) (3.1.1)\n",
      "Requirement already satisfied: pandas in /Users/amiyaguchi/.local/share/virtualenvs/wiki-forecast-HU65IRGn/lib/python3.6/site-packages (from wiki-forecast==0.3.3) (0.25.1)\n",
      "Requirement already satisfied: graphframes in /Users/amiyaguchi/.local/share/virtualenvs/wiki-forecast-HU65IRGn/lib/python3.6/site-packages (from wiki-forecast==0.3.3) (0.6)\n",
      "Requirement already satisfied: py4j==0.10.7 in /Users/amiyaguchi/.local/share/virtualenvs/wiki-forecast-HU65IRGn/lib/python3.6/site-packages (from pyspark>=2.4.0->wiki-forecast==0.3.3) (0.10.7)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /Users/amiyaguchi/.local/share/virtualenvs/wiki-forecast-HU65IRGn/lib/python3.6/site-packages (from networkx->wiki-forecast==0.3.3) (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /Users/amiyaguchi/.local/share/virtualenvs/wiki-forecast-HU65IRGn/lib/python3.6/site-packages (from matplotlib->wiki-forecast==0.3.3) (1.15.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/amiyaguchi/.local/share/virtualenvs/wiki-forecast-HU65IRGn/lib/python3.6/site-packages (from matplotlib->wiki-forecast==0.3.3) (2.8.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/amiyaguchi/.local/share/virtualenvs/wiki-forecast-HU65IRGn/lib/python3.6/site-packages (from matplotlib->wiki-forecast==0.3.3) (2.4.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/amiyaguchi/.local/share/virtualenvs/wiki-forecast-HU65IRGn/lib/python3.6/site-packages (from matplotlib->wiki-forecast==0.3.3) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/amiyaguchi/.local/share/virtualenvs/wiki-forecast-HU65IRGn/lib/python3.6/site-packages (from matplotlib->wiki-forecast==0.3.3) (0.10.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/amiyaguchi/.local/share/virtualenvs/wiki-forecast-HU65IRGn/lib/python3.6/site-packages (from pandas->wiki-forecast==0.3.3) (2019.2)\n",
      "Requirement already satisfied: nose in /Users/amiyaguchi/.local/share/virtualenvs/wiki-forecast-HU65IRGn/lib/python3.6/site-packages (from graphframes->wiki-forecast==0.3.3) (1.3.7)\n",
      "Requirement already satisfied: six>=1.5 in /Users/amiyaguchi/.local/share/virtualenvs/wiki-forecast-HU65IRGn/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib->wiki-forecast==0.3.3) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /Users/amiyaguchi/.local/share/virtualenvs/wiki-forecast-HU65IRGn/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib->wiki-forecast==0.3.3) (41.2.0)\n",
      "Building wheels for collected packages: wiki-forecast\n",
      "  Building wheel for wiki-forecast (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wiki-forecast: filename=wiki_forecast-0.3.3-cp36-none-any.whl size=27505 sha256=4704109d5996746410e950fc97dc24be551c4184a38c7a01e3de9f5c516f9a99\n",
      "  Stored in directory: /Users/amiyaguchi/Library/Caches/pip/wheels/4f/a1/40/0ace60fd57f0bb1ff87e264f714aa5255c9d115906476a9f13\n",
      "Successfully built wiki-forecast\n",
      "Installing collected packages: wiki-forecast\n",
      "  Found existing installation: wiki-forecast 0.3.3\n",
      "    Uninstalling wiki-forecast-0.3.3:\n",
      "      Successfully uninstalled wiki-forecast-0.3.3\n",
      "Successfully installed wiki-forecast-0.3.3\n"
     ]
    }
   ],
   "source": [
    "! pip install .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m../data/clustered/sample_2_8_50\u001b[00m\r\n",
      "├── [   0]  _SUCCESS\r\n",
      "├── [ 61M]  part-00000-da9ff087-c6ae-4dad-b9bd-d796a7ac0c15-c000.snappy.parquet\r\n",
      "├── [ 61M]  part-00001-da9ff087-c6ae-4dad-b9bd-d796a7ac0c15-c000.snappy.parquet\r\n",
      "├── [ 61M]  part-00002-da9ff087-c6ae-4dad-b9bd-d796a7ac0c15-c000.snappy.parquet\r\n",
      "└── [ 61M]  part-00003-da9ff087-c6ae-4dad-b9bd-d796a7ac0c15-c000.snappy.parquet\r\n",
      "\r\n",
      "0 directories, 5 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree -h ../data/clustered/sample_2_8_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- bias: boolean (nullable = true)\n",
      " |-- sign_0: boolean (nullable = true)\n",
      " |-- sign_1: boolean (nullable = true)\n",
      " |-- sign_2: boolean (nullable = true)\n",
      " |-- sign_3: boolean (nullable = true)\n",
      " |-- sign_4: boolean (nullable = true)\n",
      " |-- sign_5: boolean (nullable = true)\n",
      " |-- sign_6: boolean (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- is_redirect: boolean (nullable = true)\n",
      " |-- is_new: boolean (nullable = true)\n",
      " |-- fiedler_0: double (nullable = true)\n",
      " |-- fiedler_1: double (nullable = true)\n",
      " |-- fiedler_2: double (nullable = true)\n",
      " |-- fiedler_3: double (nullable = true)\n",
      " |-- fiedler_4: double (nullable = true)\n",
      " |-- fiedler_5: double (nullable = true)\n",
      " |-- fiedler_6: double (nullable = true)\n",
      " |-- fiedler_7: double (nullable = true)\n",
      " |-- sign_7: boolean (nullable = true)\n",
      "\n",
      "root\n",
      " |-- from: integer (nullable = true)\n",
      " |-- dest: integer (nullable = true)\n",
      "\n",
      "graph has 2936436 articles and 52893186 hyperlinks\n",
      "-RECORD 0---------------------------\n",
      " id          | 12                   \n",
      " bias        | true                 \n",
      " sign_0      | false                \n",
      " sign_1      | false                \n",
      " sign_2      | true                 \n",
      " sign_3      | false                \n",
      " sign_4      | false                \n",
      " sign_5      | false                \n",
      " sign_6      | false                \n",
      " title       | Agricultural_science \n",
      " is_redirect | false                \n",
      " is_new      | false                \n",
      " fiedler_0   | -1.83298831342705... \n",
      " fiedler_1   | -6.39406017715894... \n",
      " fiedler_2   | 1.480851908475792... \n",
      " fiedler_3   | -4.17936474134701... \n",
      " fiedler_4   | -2.09565055739335... \n",
      " fiedler_5   | -8.33324575027700... \n",
      " fiedler_6   | -9.31626502565579... \n",
      " fiedler_7   | -1.36551920482898... \n",
      " sign_7      | false                \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F, Window\n",
    "from graphframes import GraphFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wikicast.bipartition import induce_graph\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def sign_to_string(*signs):\n",
    "    result = \"\"\n",
    "    for sign in signs:\n",
    "        result += \"x\" if sign else \"o\"\n",
    "    return result\n",
    "\n",
    "def get_name(t): \n",
    "    return (start_date + timedelta(t)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "def transform(graph, pageviews, dates):\n",
    "    sign = [f\"sign_{x}\" for x in range(8)]\n",
    "    fiedler = [f\"fiedler_{x}\" for x in range(8)]     \n",
    "    time_series = (\n",
    "        pageviews.join(\n",
    "            pageviews.groupBy(\"page_id\").agg(F.sum(\"count\").alias(\"total\")).where(\"total >= 10\").select(\"page_id\"), \n",
    "            on=\"page_id\",\n",
    "            how=\"inner\"\n",
    "        )\n",
    "        .withColumn(\"id\", F.col(\"page_id\"))\n",
    "        .groupBy(\"id\")\n",
    "        .pivot(\"date\")\n",
    "        .agg(F.min(\"count\"))\n",
    "    )\n",
    "    for date in set(dates) - set(time_series.columns):\n",
    "        time_series = time_series.withColumn(date, F.lit(0))\n",
    "    return (\n",
    "        graph.vertices.join(graph.degrees, on=\"id\", how=\"left\")\n",
    "        .join(graph.inDegrees, on=\"id\", how=\"left\")\n",
    "        .join(graph.outDegrees, on=\"id\", how=\"left\")\n",
    "        .join(time_series, on=\"id\", how=\"inner\")\n",
    "        .withColumn(\"partition_id\", F.udf(sign_to_string, \"string\")(*sign))\n",
    "        .select(\"id\", \"title\", \"partition_id\", \"degree\", \"inDegree\", \"outDegree\", *sign+fiedler+dates)\n",
    "    )\n",
    "\n",
    "def plot_degree(graph):\n",
    "    degree = graph.inDegrees.groupBy(\"inDegree\").count().orderBy(F.desc(\"count\"))\n",
    "    degree.show(n=5)\n",
    "\n",
    "    df = degree.toPandas()\n",
    "    plt.scatter(df[\"inDegree\"], df[\"count\"])\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"log\")\n",
    "\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "pages = spark.read.parquet(\"../data/clustered/sample_1_8_50\")\n",
    "pagelinks = spark.read.parquet(\"../data/enwiki/pagelinks\")\n",
    "pageviews = spark.read.parquet(\"../data/enwiki/pagecount_daily_v2\")\n",
    "\n",
    "pages.printSchema()\n",
    "pagelinks.printSchema()\n",
    "\n",
    "graph = induce_graph(GraphFrame(pages, pagelinks.selectExpr(\"from as src\", \"dest as dst\")), relabel=False)\n",
    "graph.cache()\n",
    "\n",
    "print(f\"graph has {graph.vertices.count()} articles and {graph.edges.count()} hyperlinks\")\n",
    "graph.vertices.show(vertical=True, n=1)\n",
    "\n",
    "if False:\n",
    "    plot_degree(graph)\n",
    "\n",
    "start_date = datetime.strptime(\"2018-01-01\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2019-09-01\", \"%Y-%m-%d\")\n",
    "dates = [get_name(t) for t in range((end_date-start_date).days)]\n",
    "data = transform(graph, pageviews, dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.4 ms, sys: 37.1 ms, total: 62.6 ms\n",
      "Wall time: 3min 56s\n"
     ]
    }
   ],
   "source": [
    "%time data.repartitionByRange(1, \"partition_id\").write.parquet(\"../data/design_matrix/sample_1_8_50\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34msample_10_8_50\u001b[m\u001b[m \u001b[34msample_2_8_50\u001b[m\u001b[m  \u001b[34msample_4_8_50\u001b[m\u001b[m  \u001b[34msample_6_8_50\u001b[m\u001b[m  \u001b[34msample_8_8_50\u001b[m\u001b[m\r\n",
      "\u001b[34msample_1_8_50\u001b[m\u001b[m  \u001b[34msample_3_8_50\u001b[m\u001b[m  \u001b[34msample_5_8_50\u001b[m\u001b[m  \u001b[34msample_7_8_50\u001b[m\u001b[m  \u001b[34msample_9_8_50\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../data/clustered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n"
     ]
    }
   ],
   "source": [
    "def transform_batch(spark, pagelinks, pageviews, dates):\n",
    "    for i in range(2,11):\n",
    "        print(f\"iteration {i}\")\n",
    "        pages = spark.read.parquet(f\"../data/clustered/sample_{i}_8_50\")\n",
    "        graph = induce_graph(GraphFrame(pages, pagelinks.selectExpr(\"from as src\", \"dest as dst\")), relabel=False)\n",
    "        data = transform(graph, pageviews, dates)\n",
    "        (\n",
    "            data\n",
    "            .repartitionByRange(1, \"partition_id\")\n",
    "            .write\n",
    "            .parquet(f\"../data/design_matrix/sample_{i+1}_8_50\", mode=\"overwrite\")\n",
    "        )\n",
    "\n",
    "pagelinks.cache()\n",
    "pageviews.cache()\n",
    "transform_batch(spark, pagelinks, pageviews, dates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
